{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fh0SRP53qfa5"
   },
   "outputs": [],
   "source": [
    "#Here we import the necessary packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.applications import EfficientNetB1, MobileNet,MobileNetV2,NASNetMobile\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from matplotlib import pyplot \n",
    "from sklearn.model_selection import train_test_split # to split our train data into train and validation sets\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "np.random.seed(13)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mwy21xjnqi47"
   },
   "outputs": [],
   "source": [
    "#here we define a function which creates a model using base models (the transfer models)\n",
    "def create_model(base_model, model_name):\n",
    "   model = tf.keras.Sequential([base_model,\n",
    "                                 tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                 tf.keras.layers.Dropout(0.2),\n",
    "                                 tf.keras.layers.Dense(2, activation=\"softmax\")                                     \n",
    "                                ])\n",
    "   print(model_name + \" Network Architecture:\")\n",
    "   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   #plot_model(model, to_file= model_name + '.png', show_shapes=True, show_layer_names=True)\n",
    "   return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD_i8nycILGk"
   },
   "source": [
    "**Base models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkjgL_oaITVK",
    "outputId": "ce129232-6b50-458e-9a43-dd334991b0d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-mobile-no-top.h5\n",
      "19996672/19993432 [==============================] - 5s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 4s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 4s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
      "27025408/27018416 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Here we define the transfer models (base models) and set the necessary parameters\n",
    "\n",
    "InputShape = (224,224,3) # the shape of the images \n",
    "# Include_top lets you select if you want the final dense layers or not.\n",
    "NasNetBase = NASNetMobile(include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=InputShape,\n",
    "    pooling=None,\n",
    "    classes=2)\n",
    "# Freezing(i.e setting trainable=False) will prevent the weights in our base model from being updated during training.\n",
    "NasNetBase.trainable = True\n",
    "\n",
    "\n",
    "MobileNetV2Base = MobileNetV2(include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    alpha=1.0,\n",
    "    input_tensor=None,\n",
    "    input_shape=InputShape,\n",
    "    pooling=None,\n",
    "    classes=2,\n",
    "    classifier_activation=\"softmax\")\n",
    "MobileNetV2Base.trainable = True\n",
    "\n",
    "\n",
    "MobileNetBase = MobileNet(include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    input_tensor=None,\n",
    "    input_shape=InputShape,\n",
    "    pooling=None,\n",
    "    classes=2,\n",
    "    classifier_activation=\"softmax\")\n",
    "MobileNetBase.trainable = True\n",
    "\n",
    "\n",
    "EfficientNetBase = EfficientNetB1(include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=InputShape,\n",
    "    pooling=None,\n",
    "    classes=2,\n",
    "    classifier_activation=\"softmax\")\n",
    "EfficientNetBase.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4I8qQhnLu09"
   },
   "source": [
    "**THE MAIN MODELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "As9J3eCPyYjx",
    "outputId": "1bb22493-dcd0-41ee-b7f6-3510c347e11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet Network Architecture:\n",
      "NasNetModel Network Architecture:\n",
      "MobileNet Network Architecture:\n",
      "MobileNetV2 Network Architecture:\n"
     ]
    }
   ],
   "source": [
    "#Here the models are created using the pre-defined function\n",
    "EfficientNet = create_model(EfficientNetBase, 'EfficientNet')\n",
    "NasNet = create_model(NasNetBase,'NasNetModel')\n",
    "MobileNet = create_model(MobileNetBase, 'MobileNet')\n",
    "MobileNetV2 = create_model(MobileNetV2Base, 'MobileNetV2')\n",
    "\n",
    "Models = [NasNet,MobileNet,MobileNetV2,EfficientNet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0DeKQF2BqooZ",
    "outputId": "c27210de-999c-49d4-b141-f3ea8b1e75bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3929 entries, 0 to 3928\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   image_path  3929 non-null   object\n",
      " 1   mask_path   3929 non-null   object\n",
      " 2   diagnosis   3929 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 92.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#Changed to working directory to the location of the dataset and loaded the dataset description for local host.\n",
    "os.chdir(\"..\\kaggle_3m\")\n",
    "brain_df = pd.read_csv('data_frame.csv')\n",
    "\n",
    "#For google colab\n",
    "#brain_df = pd.read_csv('/content/drive/MyDrive/kaggle_3m/Gdrive.csv')\n",
    "# Here we decided to drop unncessary coloums from the dataset\n",
    "brain_df_train = brain_df.drop(columns=['Unnamed: 0', 'patient'], axis=1)\n",
    "brain_df_train['diagnosis'] = brain_df['diagnosis'].apply(lambda x: str(x)) #changes the type of the values of the column to sting\n",
    "brain_df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNjgcd0RNn2O",
    "outputId": "e8535e1c-5f94-4c43-fe44-761373e1b277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3006 validated image filenames belonging to 2 classes.\n",
      "Found 333 validated image filenames belonging to 2 classes.\n",
      "Found 590 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(brain_df_train, test_size=0.15) #splits the data into training and testing sets\n",
    "datagen = ImageDataGenerator(rescale=1./255., validation_split=0.1)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(train,\n",
    "                                              directory='./',\n",
    "                                              x_col='image_path',\n",
    "                                              y_col='diagnosis',\n",
    "                                              subset='training',\n",
    "                                              class_mode='categorical',\n",
    "                                              batch_size=16,\n",
    "                                              shuffle=True,\n",
    "                                              target_size=(224,224)\n",
    "                                             )\n",
    "valid_generator = datagen.flow_from_dataframe(train,\n",
    "                                              directory='./',\n",
    "                                              x_col='image_path',\n",
    "                                              y_col='diagnosis',\n",
    "                                              subset='validation',\n",
    "                                              class_mode='categorical',\n",
    "                                              batch_size=16,\n",
    "                                              shuffle=True,\n",
    "                                              target_size=(224,224)\n",
    "                                             )\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "test_generator = test_datagen.flow_from_dataframe(test,\n",
    "                                                  directory='./',\n",
    "                                                  x_col='image_path',\n",
    "                                                  y_col='diagnosis',\n",
    "                                                  class_mode='categorical',\n",
    "                                                  batch_size=16,\n",
    "                                                  shuffle=False,\n",
    "                                                  target_size=(224,224)\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dwyf1-mLOP8a"
   },
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor='val_loss', \n",
    "                              mode='min', \n",
    "                              verbose=1, \n",
    "                              patience=15\n",
    "                             )\n",
    "checkpointer = ModelCheckpoint(filepath=\"esemble-weights_{epoch:02d}-{val_loss:.2f}.hdf5\",\n",
    "                               verbose=1, \n",
    "                               save_best_only=True\n",
    "                              )\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              mode='min',\n",
    "                              verbose=1,\n",
    "                              patience=10,\n",
    "                              min_delta=0.0001,\n",
    "                              factor=0.2\n",
    "                             )\n",
    "callbacks = [checkpointer, earlystopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "-U_PPik1OcOv",
    "outputId": "db9e15bc-e955-4c13-da47-33d7eb453482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "187/187 [==============================] - 681s 3s/step - loss: 0.3191 - accuracy: 0.8712 - val_loss: 1.2809 - val_accuracy: 0.8562\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.28086, saving model to esemble-weights.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "187/187 [==============================] - 643s 3s/step - loss: 0.1486 - accuracy: 0.9478 - val_loss: 1.5245 - val_accuracy: 0.8781\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.28086\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - 600s 3s/step - loss: 0.1026 - accuracy: 0.9666 - val_loss: 1.2642 - val_accuracy: 0.9062\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.28086 to 1.26420, saving model to esemble-weights.hdf5\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - 614s 3s/step - loss: 0.0887 - accuracy: 0.9702 - val_loss: 2.2395 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.26420\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - 682s 4s/step - loss: 0.0657 - accuracy: 0.9789 - val_loss: 6.3449 - val_accuracy: 0.8031\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.26420\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - 673s 4s/step - loss: 0.0566 - accuracy: 0.9803 - val_loss: 2.2716 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.26420\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - 703s 4s/step - loss: 0.0385 - accuracy: 0.9886 - val_loss: 1.2356 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.26420 to 1.23563, saving model to esemble-weights.hdf5\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - 693s 4s/step - loss: 0.0968 - accuracy: 0.9712 - val_loss: 2.2986 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.23563\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - 678s 4s/step - loss: 0.0356 - accuracy: 0.9886 - val_loss: 3.5779 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.23563\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - 682s 4s/step - loss: 0.0422 - accuracy: 0.9873 - val_loss: 4.9711 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.23563\n",
      "Epoch 1/10\n",
      "187/187 [==============================] - 485s 3s/step - loss: 0.4240 - accuracy: 0.8552 - val_loss: 0.2284 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.23563 to 0.22843, saving model to esemble-weights.hdf5\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - 470s 3s/step - loss: 0.1836 - accuracy: 0.9408 - val_loss: 0.4612 - val_accuracy: 0.8906\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.22843\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - 468s 3s/step - loss: 0.1337 - accuracy: 0.9542 - val_loss: 0.1991 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.22843 to 0.19906, saving model to esemble-weights.hdf5\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - 469s 3s/step - loss: 0.0812 - accuracy: 0.9719 - val_loss: 0.2125 - val_accuracy: 0.9406\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.19906\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - 470s 3s/step - loss: 0.0818 - accuracy: 0.9699 - val_loss: 0.1726 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.19906 to 0.17264, saving model to esemble-weights.hdf5\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - 378s 2s/step - loss: 0.0557 - accuracy: 0.9819 - val_loss: 0.2409 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.17264\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - 381s 2s/step - loss: 0.0765 - accuracy: 0.9769 - val_loss: 0.1576 - val_accuracy: 0.9594\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.17264 to 0.15759, saving model to esemble-weights.hdf5\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - 376s 2s/step - loss: 0.0619 - accuracy: 0.9793 - val_loss: 0.2279 - val_accuracy: 0.9281\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.15759\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - 411s 2s/step - loss: 0.0746 - accuracy: 0.9749 - val_loss: 0.2084 - val_accuracy: 0.9406\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.15759\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - 470s 3s/step - loss: 0.0434 - accuracy: 0.9880 - val_loss: 0.1702 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.15759\n",
      "Epoch 1/10\n",
      "187/187 [==============================] - 436s 2s/step - loss: 0.3348 - accuracy: 0.8632 - val_loss: 2.2433 - val_accuracy: 0.7281\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.15759\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - 391s 2s/step - loss: 0.1993 - accuracy: 0.9314 - val_loss: 5.3416 - val_accuracy: 0.5219\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.15759\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - 394s 2s/step - loss: 0.1616 - accuracy: 0.9458 - val_loss: 5.2048 - val_accuracy: 0.3906\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.15759\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - 392s 2s/step - loss: 0.1226 - accuracy: 0.9639 - val_loss: 2.6550 - val_accuracy: 0.4156\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.15759\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - 386s 2s/step - loss: 0.1154 - accuracy: 0.9639 - val_loss: 5.1141 - val_accuracy: 0.4187\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.15759\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - 441s 2s/step - loss: 0.0844 - accuracy: 0.9746 - val_loss: 8.7860 - val_accuracy: 0.4094\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.15759\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - 444s 2s/step - loss: 0.0655 - accuracy: 0.9796 - val_loss: 2.6183 - val_accuracy: 0.5875\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.15759\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - 391s 2s/step - loss: 0.0827 - accuracy: 0.9722 - val_loss: 2.1239 - val_accuracy: 0.7875\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.15759\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - 390s 2s/step - loss: 0.0528 - accuracy: 0.9839 - val_loss: 1.4867 - val_accuracy: 0.3781\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.15759\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - 400s 2s/step - loss: 0.0977 - accuracy: 0.9749 - val_loss: 0.7210 - val_accuracy: 0.8344\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.15759\n",
      "Epoch 1/10\n",
      "187/187 [==============================] - 893s 5s/step - loss: 0.3113 - accuracy: 0.8699 - val_loss: 1.8866 - val_accuracy: 0.6719\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.15759\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - 864s 5s/step - loss: 0.1842 - accuracy: 0.9395 - val_loss: 0.9838 - val_accuracy: 0.6656\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.15759\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - 875s 5s/step - loss: 0.1624 - accuracy: 0.9421 - val_loss: 0.7620 - val_accuracy: 0.6625\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.15759\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - 873s 5s/step - loss: 0.1309 - accuracy: 0.9559 - val_loss: 1.9639 - val_accuracy: 0.6656\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.15759\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - 872s 5s/step - loss: 0.1100 - accuracy: 0.9645 - val_loss: 2.4445 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.15759\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - 954s 5s/step - loss: 0.0797 - accuracy: 0.9736 - val_loss: 1.4983 - val_accuracy: 0.6656\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.15759\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - 1067s 6s/step - loss: 0.0682 - accuracy: 0.9766 - val_loss: 1.1413 - val_accuracy: 0.6656\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.15759\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - 1107s 6s/step - loss: 0.0640 - accuracy: 0.9803 - val_loss: 3.0498 - val_accuracy: 0.6656\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.15759\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - 1087s 6s/step - loss: 0.0486 - accuracy: 0.9836 - val_loss: 2.6869 - val_accuracy: 0.6687\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.15759\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - 1034s 6s/step - loss: 0.0365 - accuracy: 0.9853 - val_loss: 1.7573 - val_accuracy: 0.6687\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.15759\n"
     ]
    }
   ],
   "source": [
    "TrainedModel = []\n",
    "for model in Models:\n",
    "  model.fit(train_generator,steps_per_epoch= train_generator.n // train_generator.batch_size,epochs = 10,validation_data= valid_generator,validation_steps= valid_generator.n // valid_generator.batch_size,callbacks=[checkpointer, earlystopping])\n",
    "  TrainedModel.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dE3FADdwP3e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 1 1 0]\n",
      " ...\n",
      " [1 1 1 0]\n",
      " [1 1 1 0]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#Save the models\n",
    "#if not os.path.exists('my_models'):\n",
    "#    os.makedirs('my_models')\n",
    "#for i in range(len(TrainedModel)):\n",
    "#    TrainedModel[i].save('my_models/'+str(TrainedModel[i])+str(i+1))    \n",
    "# Predict labels with models\n",
    "\n",
    "labels = []\n",
    "for m in TrainedModel:\n",
    "    predicts = np.argmax(m.predict(test_generator), axis=1)\n",
    "    labels.append(predicts)\n",
    "\n",
    "# Ensemble with voting\n",
    "labels = np.array(labels)\n",
    "labels = np.transpose(labels, (1, 0))\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labls = scipy.stats.mode(labels,axis=1)[0]\n",
    "labls = np.squeeze(labls)\n",
    "\n",
    "\n",
    "original = np.asarray(test['diagnosis']).astype('int')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1\n",
      " 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0\n",
      " 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0\n",
      " 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0\n",
      " 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1\n",
      " 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 0 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "5iIXwwjLP4xa"
   },
   "outputs": [],
   "source": [
    "# Dump predictions into submission file\n",
    "pd.DataFrame({'ImageId' : np.arange(1, labels.shape[0] + 1), 'Label' : labls }).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "SMyhFWMqH9k8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9271186440677966\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95       380\n",
      "           1       0.98      0.81      0.89       210\n",
      "\n",
      "    accuracy                           0.93       590\n",
      "   macro avg       0.94      0.90      0.92       590\n",
      "weighted avg       0.93      0.93      0.93       590\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "pred = labls\n",
    "accuracy = accuracy_score(original, pred)\n",
    "print(accuracy)\n",
    "\n",
    "cm = confusion_matrix(original, pred)\n",
    "\n",
    "report = classification_report(original, pred, labels = [0,1])\n",
    "print(report)\n",
    "pyplot.figure(figsize = (5,5))\n",
    "sns.heatmap(cm, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ihmhYXzNl5a"
   },
   "source": [
    "**Saving The Trained Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "bYjYFmCvIG10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) NASNet_input with unsupported characters which will be renamed to nasnet_input in the SavedModel.\n",
      "c:\\users\\hi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_firstModel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_firstModel\\assets\n"
     ]
    }
   ],
   "source": [
    "TrainedModel[0].save('my_firstModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenet_1.00_224_input with unsupported characters which will be renamed to mobilenet_1_00_224_input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_secondModel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_secondModel\\assets\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_224_input with unsupported characters which will be renamed to mobilenetv2_1_00_224_input in the SavedModel.\n",
      "c:\\users\\hi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_thirdModel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_thirdModel\\assets\n",
      "c:\\users\\hi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_forthModel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_forthModel\\assets\n"
     ]
    }
   ],
   "source": [
    "TrainedModel[1].save('my_secondModel')\n",
    "TrainedModel[2].save('my_thirdModel')\n",
    "TrainedModel[3].save('my_forthModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TheEnsembleModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
